"""
Autonomous Analytics - Self-Analyzing Quantum Performance Intelligence
==================================================================

This module implements the autonomous analytics system that collects,
analyzes, and summarizes performance, cost, and entanglement data,
provides actionable insights and forecasts for future quantum workloads,
and self-documents discovered optimization strategies and patterns.

This is the GOD-TIER analytical intelligence that makes Coratrix
truly self-aware of its own performance and capabilities.
"""

import asyncio
import time
import logging
import numpy as np
import threading
import json
import pandas as pd
import random
from typing import Dict, List, Optional, Tuple, Any, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict, deque
import uuid
from concurrent.futures import ThreadPoolExecutor, as_completed
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import IsolationForest
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
import joblib

logger = logging.getLogger(__name__)

class AnalyticsType(Enum):
    """Types of autonomous analytics."""
    PERFORMANCE_ANALYSIS = "performance_analysis"
    COST_ANALYSIS = "cost_analysis"
    ENTANGLEMENT_ANALYSIS = "entanglement_analysis"
    PREDICTIVE_ANALYSIS = "predictive_analysis"
    OPTIMIZATION_ANALYSIS = "optimization_analysis"
    SYSTEM_HEALTH_ANALYSIS = "system_health_analysis"

class InsightType(Enum):
    """Types of analytical insights."""
    PERFORMANCE_BOTTLENECK = "performance_bottleneck"
    COST_OPTIMIZATION = "cost_optimization"
    ENTANGLEMENT_OPTIMIZATION = "entanglement_optimization"
    PREDICTIVE_FORECAST = "predictive_forecast"
    ANOMALY_DETECTION = "anomaly_detection"
    OPTIMIZATION_OPPORTUNITY = "optimization_opportunity"

@dataclass
class PerformanceMetric:
    """A performance metric with metadata."""
    metric_id: str
    timestamp: float
    metric_type: str
    value: float
    unit: str
    context: Dict[str, Any] = field(default_factory=dict)
    tags: List[str] = field(default_factory=list)

@dataclass
class AnalyticalInsight:
    """An analytical insight with recommendations."""
    insight_id: str
    timestamp: float
    insight_type: InsightType
    priority: str
    confidence: float
    description: str
    data_evidence: Dict[str, Any]
    recommendations: List[Dict[str, Any]]
    impact_estimate: Dict[str, float]
    implementation_plan: List[Dict[str, Any]] = field(default_factory=list)

@dataclass
class PredictiveForecast:
    """A predictive forecast for future performance."""
    forecast_id: str
    timestamp: float
    forecast_type: str
    time_horizon: str
    predicted_values: Dict[str, List[float]]
    confidence_intervals: Dict[str, Tuple[float, float]]
    trend_analysis: Dict[str, str]
    recommendations: List[Dict[str, Any]]

class AutonomousAnalytics:
    """
    GOD-TIER Autonomous Analytics for Self-Analyzing Quantum Performance.
    
    This analytics system continuously monitors, analyzes, and provides
    insights about quantum system performance, enabling autonomous
    optimization and strategic decision-making.
    
    This transforms Coratrix into a self-aware quantum OS that
    understands its own performance and can predict future needs.
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """Initialize the Autonomous Analytics system."""
        self.config = config or {}
        self.analytics_id = f"aa_{int(time.time() * 1000)}"
        
        # Data collection
        self.metrics_buffer: deque = deque(maxlen=100000)
        self.insights_history: deque = deque(maxlen=10000)
        self.forecasts_history: deque = deque(maxlen=1000)
        
        # Analysis models
        self.anomaly_detector = IsolationForest(contamination=0.1, random_state=42)
        self.clustering_model = DBSCAN(eps=0.5, min_samples=5)
        self.scaler = StandardScaler()
        self.models_trained = False
        
        # Performance tracking
        self.performance_metrics: Dict[str, Any] = {}
        self.system_health: Dict[str, Any] = {}
        self.cost_analysis: Dict[str, Any] = {}
        self.entanglement_analysis: Dict[str, Any] = {}
        
        # Analytics state
        self.analytics_state = {
            'data_points_collected': 0,
            'insights_generated': 0,
            'forecasts_made': 0,
            'anomalies_detected': 0,
            'optimization_opportunities': 0
        }
        
        # Threading
        self.executor = ThreadPoolExecutor(max_workers=10)
        self.running = False
        self.analytics_thread = None
        self.insight_thread = None
        
        logger.info(f"üìä Autonomous Analytics initialized (ID: {self.analytics_id})")
        logger.info("üöÄ GOD-TIER analytical intelligence active")
    
    async def start(self):
        """Start the autonomous analytics system."""
        self.running = True
        
        # Start analytics thread
        self.analytics_thread = threading.Thread(target=self._analytics_loop, daemon=True)
        self.analytics_thread.start()
        
        # Start insight generation thread
        self.insight_thread = threading.Thread(target=self._insight_loop, daemon=True)
        self.insight_thread.start()
        
        logger.info("üéØ Autonomous Analytics started")
    
    async def stop(self):
        """Stop the autonomous analytics system."""
        self.running = False
        
        if self.analytics_thread:
            self.analytics_thread.join(timeout=5.0)
        if self.insight_thread:
            self.insight_thread.join(timeout=5.0)
        
        logger.info("üõë Autonomous Analytics stopped")
    
    def _analytics_loop(self):
        """Main analytics processing loop."""
        while self.running:
            try:
                # Process collected metrics
                self._process_metrics()
                
                # Update performance analysis
                self._update_performance_analysis()
                
                # Update system health
                self._update_system_health()
                
                # Update cost analysis
                self._update_cost_analysis()
                
                # Update entanglement analysis
                self._update_entanglement_analysis()
                
                # Sleep between analytics cycles
                time.sleep(10.0)  # Analyze every 10 seconds
                
            except Exception as e:
                logger.error(f"‚ùå Analytics loop error: {e}")
                time.sleep(5.0)
    
    def _insight_loop(self):
        """Insight generation loop."""
        while self.running:
            try:
                # Generate insights
                self._generate_insights()
                
                # Generate forecasts
                self._generate_forecasts()
                
                # Detect anomalies
                self._detect_anomalies()
                
                # Sleep between insight cycles
                time.sleep(30.0)  # Generate insights every 30 seconds
                
            except Exception as e:
                logger.error(f"‚ùå Insight loop error: {e}")
                time.sleep(10.0)
    
    def _process_metrics(self):
        """Process collected metrics for analysis."""
        if len(self.metrics_buffer) < 10:
            return
        
        # Convert metrics to DataFrame for analysis
        metrics_data = []
        for metric in self.metrics_buffer:
            metrics_data.append({
                'timestamp': metric.timestamp,
                'type': metric.metric_type,
                'value': metric.value,
                'unit': metric.unit
            })
        
        if not metrics_data:
            return
        
        df = pd.DataFrame(metrics_data)
        
        # Update analytics state
        self.analytics_state['data_points_collected'] = len(self.metrics_buffer)
        
        # Train models if enough data
        if len(metrics_data) > 100 and not self.models_trained:
            self._train_analytics_models(df)
    
    def _train_analytics_models(self, df: pd.DataFrame):
        """Train analytics models on collected data."""
        try:
            # Prepare features for training
            features = self._prepare_features(df)
            
            if len(features) > 50:
                # Train anomaly detector
                self.anomaly_detector.fit(features)
                
                # Train clustering model
                self.clustering_model.fit(features)
                
                # Fit scaler
                self.scaler.fit(features)
                
                self.models_trained = True
                logger.info("üìä Analytics models trained successfully")
        
        except Exception as e:
            logger.error(f"‚ùå Model training error: {e}")
    
    def _prepare_features(self, df: pd.DataFrame) -> np.ndarray:
        """Prepare features for model training."""
        # Group by metric type and calculate statistics
        feature_data = []
        
        for metric_type in df['type'].unique():
            type_data = df[df['type'] == metric_type]
            
            if len(type_data) > 0:
                features = [
                    type_data['value'].mean(),
                    type_data['value'].std(),
                    type_data['value'].min(),
                    type_data['value'].max(),
                    len(type_data)
                ]
                feature_data.append(features)
        
        return np.array(feature_data) if feature_data else np.array([]).reshape(0, 5)
    
    def _update_performance_analysis(self):
        """Update performance analysis metrics."""
        if len(self.metrics_buffer) < 10:
            return
        
        # Calculate performance metrics
        execution_times = [m.value for m in self.metrics_buffer if m.metric_type == 'execution_time']
        memory_usage = [m.value for m in self.metrics_buffer if m.metric_type == 'memory_usage']
        cpu_usage = [m.value for m in self.metrics_buffer if m.metric_type == 'cpu_usage']
        
        self.performance_metrics = {
            'average_execution_time': np.mean(execution_times) if execution_times else 0.0,
            'execution_time_std': np.std(execution_times) if execution_times else 0.0,
            'average_memory_usage': np.mean(memory_usage) if memory_usage else 0.0,
            'average_cpu_usage': np.mean(cpu_usage) if cpu_usage else 0.0,
            'performance_trend': self._calculate_performance_trend(),
            'bottlenecks': self._identify_bottlenecks(),
            'optimization_opportunities': self._identify_optimization_opportunities()
        }
    
    def _calculate_performance_trend(self) -> str:
        """Calculate performance trend over time."""
        if len(self.metrics_buffer) < 20:
            return 'insufficient_data'
        
        # Get recent execution times
        recent_metrics = list(self.metrics_buffer)[-20:]
        execution_times = [m.value for m in recent_metrics if m.metric_type == 'execution_time']
        
        if len(execution_times) < 10:
            return 'insufficient_data'
        
        # Calculate trend
        x = np.arange(len(execution_times))
        y = np.array(execution_times)
        
        if len(y) > 1:
            slope = np.polyfit(x, y, 1)[0]
            
            if slope < -0.1:
                return 'improving'
            elif slope > 0.1:
                return 'degrading'
            else:
                return 'stable'
        
        return 'stable'
    
    def _identify_bottlenecks(self) -> List[str]:
        """Identify performance bottlenecks."""
        bottlenecks = []
        
        # Check CPU usage
        cpu_metrics = [m.value for m in self.metrics_buffer if m.metric_type == 'cpu_usage']
        if cpu_metrics and np.mean(cpu_metrics) > 80:
            bottlenecks.append('high_cpu_usage')
        
        # Check memory usage
        memory_metrics = [m.value for m in self.metrics_buffer if m.metric_type == 'memory_usage']
        if memory_metrics and np.mean(memory_metrics) > 80:
            bottlenecks.append('high_memory_usage')
        
        # Check execution time
        execution_metrics = [m.value for m in self.metrics_buffer if m.metric_type == 'execution_time']
        if execution_metrics and np.mean(execution_metrics) > 1000:  # 1 second
            bottlenecks.append('slow_execution')
        
        return bottlenecks
    
    def _identify_optimization_opportunities(self) -> List[Dict[str, Any]]:
        """Identify optimization opportunities."""
        opportunities = []
        
        # Analyze execution patterns
        execution_metrics = [m.value for m in self.metrics_buffer if m.metric_type == 'execution_time']
        if execution_metrics:
            execution_std = np.std(execution_metrics)
            execution_mean = np.mean(execution_metrics)
            
            if execution_std / execution_mean > 0.5:  # High variance
                opportunities.append({
                    'type': 'execution_optimization',
                    'description': 'High execution time variance detected',
                    'potential_improvement': 0.2
                })
        
        # Analyze memory patterns
        memory_metrics = [m.value for m in self.metrics_buffer if m.metric_type == 'memory_usage']
        if memory_metrics:
            memory_mean = np.mean(memory_metrics)
            if memory_mean > 50:  # High memory usage
                opportunities.append({
                    'type': 'memory_optimization',
                    'description': 'High memory usage detected',
                    'potential_improvement': 0.15
                })
        
        return opportunities
    
    def _update_system_health(self):
        """Update system health metrics."""
        # Calculate system health score
        health_components = {
            'cpu_health': self._calculate_cpu_health(),
            'memory_health': self._calculate_memory_health(),
            'execution_health': self._calculate_execution_health(),
            'reliability_health': self._calculate_reliability_health()
        }
        
        # Overall health score
        overall_health = np.mean(list(health_components.values()))
        
        self.system_health = {
            'overall_health': overall_health,
            'health_components': health_components,
            'health_status': self._determine_health_status(overall_health),
            'recommendations': self._generate_health_recommendations(health_components)
        }
    
    def _calculate_cpu_health(self) -> float:
        """Calculate CPU health score."""
        cpu_metrics = [m.value for m in self.metrics_buffer if m.metric_type == 'cpu_usage']
        if not cpu_metrics:
            return 1.0
        
        avg_cpu = np.mean(cpu_metrics)
        return max(0.0, 1.0 - (avg_cpu - 50) / 50)  # 50% is optimal
    
    def _calculate_memory_health(self) -> float:
        """Calculate memory health score."""
        memory_metrics = [m.value for m in self.metrics_buffer if m.metric_type == 'memory_usage']
        if not memory_metrics:
            return 1.0
        
        avg_memory = np.mean(memory_metrics)
        return max(0.0, 1.0 - (avg_memory - 50) / 50)  # 50% is optimal
    
    def _calculate_execution_health(self) -> float:
        """Calculate execution health score."""
        execution_metrics = [m.value for m in self.metrics_buffer if m.metric_type == 'execution_time']
        if not execution_metrics:
            return 1.0
        
        avg_execution = np.mean(execution_metrics)
        return max(0.0, 1.0 - (avg_execution - 100) / 900)  # 100ms is optimal
    
    def _calculate_reliability_health(self) -> float:
        """Calculate reliability health score."""
        # This would analyze error rates, success rates, etc.
        # For now, return a mock value
        return random.uniform(0.8, 1.0)
    
    def _determine_health_status(self, overall_health: float) -> str:
        """Determine overall health status."""
        if overall_health >= 0.9:
            return 'excellent'
        elif overall_health >= 0.7:
            return 'good'
        elif overall_health >= 0.5:
            return 'fair'
        else:
            return 'poor'
    
    def _generate_health_recommendations(self, health_components: Dict[str, float]) -> List[str]:
        """Generate health recommendations."""
        recommendations = []
        
        for component, score in health_components.items():
            if score < 0.7:
                if component == 'cpu_health':
                    recommendations.append('Consider CPU optimization or scaling')
                elif component == 'memory_health':
                    recommendations.append('Consider memory optimization or increase')
                elif component == 'execution_health':
                    recommendations.append('Consider execution optimization')
                elif component == 'reliability_health':
                    recommendations.append('Investigate reliability issues')
        
        return recommendations
    
    def _update_cost_analysis(self):
        """Update cost analysis metrics."""
        cost_metrics = [m.value for m in self.metrics_buffer if m.metric_type == 'cost']
        
        if cost_metrics:
            self.cost_analysis = {
                'total_cost': sum(cost_metrics),
                'average_cost': np.mean(cost_metrics),
                'cost_trend': self._calculate_cost_trend(),
                'cost_optimization_opportunities': self._identify_cost_optimization_opportunities()
            }
        else:
            self.cost_analysis = {
                'total_cost': 0.0,
                'average_cost': 0.0,
                'cost_trend': 'no_data',
                'cost_optimization_opportunities': []
            }
    
    def _calculate_cost_trend(self) -> str:
        """Calculate cost trend over time."""
        cost_metrics = [m.value for m in self.metrics_buffer if m.metric_type == 'cost']
        
        if len(cost_metrics) < 10:
            return 'insufficient_data'
        
        # Calculate trend
        x = np.arange(len(cost_metrics))
        y = np.array(cost_metrics)
        
        if len(y) > 1:
            slope = np.polyfit(x, y, 1)[0]
            
            if slope < -0.01:
                return 'decreasing'
            elif slope > 0.01:
                return 'increasing'
            else:
                return 'stable'
        
        return 'stable'
    
    def _identify_cost_optimization_opportunities(self) -> List[Dict[str, Any]]:
        """Identify cost optimization opportunities."""
        opportunities = []
        
        cost_metrics = [m.value for m in self.metrics_buffer if m.metric_type == 'cost']
        if cost_metrics:
            avg_cost = np.mean(cost_metrics)
            cost_std = np.std(cost_metrics)
            
            if cost_std / avg_cost > 0.3:  # High cost variance
                opportunities.append({
                    'type': 'cost_variance_optimization',
                    'description': 'High cost variance detected',
                    'potential_savings': 0.15
                })
            
            if avg_cost > 1.0:  # High average cost
                opportunities.append({
                    'type': 'cost_reduction_optimization',
                    'description': 'High average cost detected',
                    'potential_savings': 0.2
                })
        
        return opportunities
    
    def _update_entanglement_analysis(self):
        """Update entanglement analysis metrics."""
        entanglement_metrics = [m.value for m in self.metrics_buffer if m.metric_type == 'entanglement']
        
        if entanglement_metrics:
            self.entanglement_analysis = {
                'average_entanglement': np.mean(entanglement_metrics),
                'entanglement_variance': np.std(entanglement_metrics),
                'entanglement_patterns': self._analyze_entanglement_patterns(),
                'optimization_opportunities': self._identify_entanglement_optimization_opportunities()
            }
        else:
            self.entanglement_analysis = {
                'average_entanglement': 0.0,
                'entanglement_variance': 0.0,
                'entanglement_patterns': {},
                'optimization_opportunities': []
            }
    
    def _analyze_entanglement_patterns(self) -> Dict[str, Any]:
        """Analyze entanglement patterns."""
        # This would analyze actual entanglement data
        # For now, return mock analysis
        return {
            'linear_entanglement': random.uniform(0.1, 0.4),
            'star_entanglement': random.uniform(0.1, 0.3),
            'ring_entanglement': random.uniform(0.05, 0.2),
            'complete_entanglement': random.uniform(0.0, 0.1)
        }
    
    def _identify_entanglement_optimization_opportunities(self) -> List[Dict[str, Any]]:
        """Identify entanglement optimization opportunities."""
        opportunities = []
        
        # This would analyze actual entanglement data
        # For now, return mock opportunities
        if random.random() < 0.3:
            opportunities.append({
                'type': 'entanglement_optimization',
                'description': 'Entanglement optimization opportunity detected',
                'potential_improvement': random.uniform(0.1, 0.3)
            })
        
        return opportunities
    
    def _generate_insights(self):
        """Generate analytical insights."""
        insights = []
        
        # Performance insights
        if self.performance_metrics:
            performance_insights = self._generate_performance_insights()
            insights.extend(performance_insights)
        
        # System health insights
        if self.system_health:
            health_insights = self._generate_health_insights()
            insights.extend(health_insights)
        
        # Cost insights
        if self.cost_analysis:
            cost_insights = self._generate_cost_insights()
            insights.extend(cost_insights)
        
        # Entanglement insights
        if self.entanglement_analysis:
            entanglement_insights = self._generate_entanglement_insights()
            insights.extend(entanglement_insights)
        
        # Store insights
        for insight in insights:
            self.insights_history.append(insight)
            self.analytics_state['insights_generated'] += 1
    
    def _generate_performance_insights(self) -> List[AnalyticalInsight]:
        """Generate performance insights."""
        insights = []
        
        # Bottleneck insights
        if 'bottlenecks' in self.performance_metrics:
            bottlenecks = self.performance_metrics['bottlenecks']
            if bottlenecks:
                insight = AnalyticalInsight(
                    insight_id=f"perf_{int(time.time() * 1000)}",
                    timestamp=time.time(),
                    insight_type=InsightType.PERFORMANCE_BOTTLENECK,
                    priority='high',
                    confidence=0.8,
                    description=f"Performance bottlenecks detected: {', '.join(bottlenecks)}",
                    data_evidence={'bottlenecks': bottlenecks},
                    recommendations=[
                        {'action': 'investigate_bottlenecks', 'parameters': {'bottlenecks': bottlenecks}},
                        {'action': 'optimize_performance', 'parameters': {}}
                    ],
                    impact_estimate={'performance_improvement': 0.2}
                )
                insights.append(insight)
        
        # Optimization opportunity insights
        if 'optimization_opportunities' in self.performance_metrics:
            opportunities = self.performance_metrics['optimization_opportunities']
            if opportunities:
                insight = AnalyticalInsight(
                    insight_id=f"opt_{int(time.time() * 1000)}",
                    timestamp=time.time(),
                    insight_type=InsightType.OPTIMIZATION_OPPORTUNITY,
                    priority='medium',
                    confidence=0.7,
                    description=f"Optimization opportunities identified: {len(opportunities)}",
                    data_evidence={'opportunities': opportunities},
                    recommendations=[
                        {'action': 'implement_optimizations', 'parameters': {'opportunities': opportunities}}
                    ],
                    impact_estimate={'optimization_gain': 0.15}
                )
                insights.append(insight)
        
        return insights
    
    def _generate_health_insights(self) -> List[AnalyticalInsight]:
        """Generate system health insights."""
        insights = []
        
        if self.system_health['overall_health'] < 0.7:
            insight = AnalyticalInsight(
                insight_id=f"health_{int(time.time() * 1000)}",
                timestamp=time.time(),
                insight_type=InsightType.SYSTEM_HEALTH_ANALYSIS,
                priority='high',
                confidence=0.9,
                description=f"System health is {self.system_health['health_status']}",
                data_evidence={'health_score': self.system_health['overall_health']},
                recommendations=self.system_health['recommendations'],
                impact_estimate={'health_improvement': 0.3}
            )
            insights.append(insight)
        
        return insights
    
    def _generate_cost_insights(self) -> List[AnalyticalInsight]:
        """Generate cost insights."""
        insights = []
        
        if self.cost_analysis.get('cost_trend') == 'increasing':
            insight = AnalyticalInsight(
                insight_id=f"cost_{int(time.time() * 1000)}",
                timestamp=time.time(),
                insight_type=InsightType.COST_OPTIMIZATION,
                priority='medium',
                confidence=0.7,
                description="Cost trend is increasing",
                data_evidence={'cost_trend': self.cost_analysis['cost_trend']},
                recommendations=[
                    {'action': 'analyze_cost_drivers', 'parameters': {}},
                    {'action': 'implement_cost_optimizations', 'parameters': {}}
                ],
                impact_estimate={'cost_reduction': 0.2}
            )
            insights.append(insight)
        
        return insights
    
    def _generate_entanglement_insights(self) -> List[AnalyticalInsight]:
        """Generate entanglement insights."""
        insights = []
        
        # This would generate insights based on actual entanglement analysis
        # For now, return mock insights
        if random.random() < 0.2:  # 20% chance of insight
            insight = AnalyticalInsight(
                insight_id=f"ent_{int(time.time() * 1000)}",
                timestamp=time.time(),
                insight_type=InsightType.ENTANGLEMENT_OPTIMIZATION,
                priority='low',
                confidence=0.6,
                description="Entanglement optimization opportunity detected",
                data_evidence={'entanglement_analysis': self.entanglement_analysis},
                recommendations=[
                    {'action': 'optimize_entanglement', 'parameters': {}}
                ],
                impact_estimate={'entanglement_improvement': 0.1}
            )
            insights.append(insight)
        
        return insights
    
    def _generate_forecasts(self):
        """Generate predictive forecasts."""
        if len(self.metrics_buffer) < 100:
            return
        
        # Generate performance forecast
        performance_forecast = self._generate_performance_forecast()
        if performance_forecast:
            self.forecasts_history.append(performance_forecast)
            self.analytics_state['forecasts_made'] += 1
        
        # Generate cost forecast
        cost_forecast = self._generate_cost_forecast()
        if cost_forecast:
            self.forecasts_history.append(cost_forecast)
            self.analytics_state['forecasts_made'] += 1
    
    def _generate_performance_forecast(self) -> Optional[PredictiveForecast]:
        """Generate performance forecast."""
        execution_metrics = [m.value for m in self.metrics_buffer if m.metric_type == 'execution_time']
        
        if len(execution_metrics) < 50:
            return None
        
        # Simple linear forecast
        x = np.arange(len(execution_metrics))
        y = np.array(execution_metrics)
        
        # Fit linear trend
        coeffs = np.polyfit(x, y, 1)
        
        # Forecast next 10 points
        future_x = np.arange(len(execution_metrics), len(execution_metrics) + 10)
        future_y = coeffs[0] * future_x + coeffs[1]
        
        return PredictiveForecast(
            forecast_id=f"perf_forecast_{int(time.time() * 1000)}",
            timestamp=time.time(),
            forecast_type='performance',
            time_horizon='short_term',
            predicted_values={'execution_time': future_y.tolist()},
            confidence_intervals={'execution_time': (future_y.min(), future_y.max())},
            trend_analysis={'trend': 'increasing' if coeffs[0] > 0 else 'decreasing'},
            recommendations=[
                {'action': 'monitor_performance', 'parameters': {}},
                {'action': 'optimize_if_needed', 'parameters': {}}
            ]
        )
    
    def _generate_cost_forecast(self) -> Optional[PredictiveForecast]:
        """Generate cost forecast."""
        cost_metrics = [m.value for m in self.metrics_buffer if m.metric_type == 'cost']
        
        if len(cost_metrics) < 50:
            return None
        
        # Simple linear forecast
        x = np.arange(len(cost_metrics))
        y = np.array(cost_metrics)
        
        # Fit linear trend
        coeffs = np.polyfit(x, y, 1)
        
        # Forecast next 10 points
        future_x = np.arange(len(cost_metrics), len(cost_metrics) + 10)
        future_y = coeffs[0] * future_x + coeffs[1]
        
        return PredictiveForecast(
            forecast_id=f"cost_forecast_{int(time.time() * 1000)}",
            timestamp=time.time(),
            forecast_type='cost',
            time_horizon='short_term',
            predicted_values={'cost': future_y.tolist()},
            confidence_intervals={'cost': (future_y.min(), future_y.max())},
            trend_analysis={'trend': 'increasing' if coeffs[0] > 0 else 'decreasing'},
            recommendations=[
                {'action': 'monitor_costs', 'parameters': {}},
                {'action': 'optimize_costs', 'parameters': {}}
            ]
        )
    
    def _detect_anomalies(self):
        """Detect anomalies in the data."""
        if not self.models_trained or len(self.metrics_buffer) < 100:
            return
        
        try:
            # Prepare features for anomaly detection
            features = self._prepare_features_for_anomaly_detection()
            
            if len(features) > 0:
                # Detect anomalies
                anomaly_scores = self.anomaly_detector.decision_function(features)
                anomalies = self.anomaly_detector.predict(features)
                
                # Count anomalies
                num_anomalies = sum(1 for a in anomalies if a == -1)
                if num_anomalies > 0:
                    self.analytics_state['anomalies_detected'] += num_anomalies
                    
                    # Generate anomaly insights
                    anomaly_insight = AnalyticalInsight(
                        insight_id=f"anomaly_{int(time.time() * 1000)}",
                        timestamp=time.time(),
                        insight_type=InsightType.ANOMALY_DETECTION,
                        priority='high',
                        confidence=0.8,
                        description=f"Detected {num_anomalies} anomalies in system behavior",
                        data_evidence={'anomaly_count': num_anomalies, 'anomaly_scores': anomaly_scores.tolist()},
                        recommendations=[
                            {'action': 'investigate_anomalies', 'parameters': {'anomaly_count': num_anomalies}}
                        ],
                        impact_estimate={'anomaly_resolution': 0.3}
                    )
                    
                    self.insights_history.append(anomaly_insight)
                    self.analytics_state['insights_generated'] += 1
        
        except Exception as e:
            logger.error(f"‚ùå Anomaly detection error: {e}")
    
    def _prepare_features_for_anomaly_detection(self) -> np.ndarray:
        """Prepare features for anomaly detection."""
        # Get recent metrics
        recent_metrics = list(self.metrics_buffer)[-100:]
        
        # Group by type and calculate features
        feature_data = []
        for metric_type in set(m.metric_type for m in recent_metrics):
            type_metrics = [m.value for m in recent_metrics if m.metric_type == metric_type]
            if type_metrics:
                features = [
                    np.mean(type_metrics),
                    np.std(type_metrics),
                    np.min(type_metrics),
                    np.max(type_metrics),
                    len(type_metrics)
                ]
                feature_data.append(features)
        
        return np.array(feature_data) if feature_data else np.array([]).reshape(0, 5)
    
    def collect_metric(self, metric_type: str, value: float, unit: str, 
                      context: Dict[str, Any] = None, tags: List[str] = None):
        """Collect a performance metric."""
        metric = PerformanceMetric(
            metric_id=f"metric_{int(time.time() * 1000)}",
            timestamp=time.time(),
            metric_type=metric_type,
            value=value,
            unit=unit,
            context=context or {},
            tags=tags or []
        )
        
        self.metrics_buffer.append(metric)
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """Get current performance metrics."""
        return self.performance_metrics.copy()
    
    def get_system_health(self) -> Dict[str, Any]:
        """Get current system health."""
        return self.system_health.copy()
    
    def get_analytical_insights(self) -> List[Dict[str, Any]]:
        """Get recent analytical insights."""
        return [
            {
                'insight_id': i.insight_id,
                'timestamp': i.timestamp,
                'insight_type': i.insight_type.value,
                'priority': i.priority,
                'confidence': i.confidence,
                'description': i.description,
                'recommendations': i.recommendations,
                'impact_estimate': i.impact_estimate
            }
            for i in list(self.insights_history)[-20:]
        ]
    
    def get_predictive_forecasts(self) -> List[Dict[str, Any]]:
        """Get recent predictive forecasts."""
        return [
            {
                'forecast_id': f.forecast_id,
                'timestamp': f.timestamp,
                'forecast_type': f.forecast_type,
                'time_horizon': f.time_horizon,
                'predicted_values': f.predicted_values,
                'trend_analysis': f.trend_analysis,
                'recommendations': f.recommendations
            }
            for f in list(self.forecasts_history)[-10:]
        ]
    
    def get_analytics_report(self) -> Dict[str, Any]:
        """Generate comprehensive analytics report."""
        return {
            'timestamp': time.time(),
            'analytics_state': self.analytics_state.copy(),
            'performance_metrics': self.performance_metrics.copy(),
            'system_health': self.system_health.copy(),
            'cost_analysis': self.cost_analysis.copy(),
            'entanglement_analysis': self.entanglement_analysis.copy(),
            'recent_insights': self.get_analytical_insights(),
            'recent_forecasts': self.get_predictive_forecasts(),
            'models_trained': self.models_trained
        }
